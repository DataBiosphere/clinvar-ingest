apiVersion: argoproj.io/v1alpha1
kind: WorkflowTemplate
metadata:
  name: process-and-reingest-release
spec:
  entrypoint: main
  serviceAccountName: {{ .Values.serviceAccount.k8sName }}
  templates:
    {{- $pipelineVersion := default "latest" .Values.version }}
    {{- $versionIsPinned := ne $pipelineVersion "latest" }}

    ##
    ## Entrypoint for processing a ClinVar archive.
    ##
    ## Converts the archive to JSON and processes it with Dataflow,
    ## then pushes the result into the TDR.
    ##
    - name: main
      inputs:
        parameters:
          - name: release-date
          {{- $releaseDate := "{{inputs.parameters.release-date}}" }}
          - name: gcs-prefix
          {{- $gcsPrefix := "{{inputs.parameters.gcs-prefix}}" }}
      dag:
        tasks:
          # Query the processing_history table to see if we've already
          # done the work to ingest the release-date under the current pipeline
          # version.
          #
          # This is trickier than it sounds because our current dev setup causes
          # the pipeline version to always be "latest", which isn't very useful.
          #
          # For now we assume that "latest" means we should always reprocess the
          # data, and disable auto-snapshot in that case.
          {{- if $versionIsPinned }}
          - name: check-processing-history
            template: get-processing-history-rows
            arguments:
              parameters:
                - name: release-date
                  value: {{ $releaseDate | quote }}
          {{- end }}
          {{- $processingNeeded := "{{tasks.check-processing-history.outputs.result}} == 0" }}

          - name: process-archive
            {{- if $versionIsPinned }}
            dependencies: [check-processing-history]
            when: {{ $processingNeeded | quote }}
            {{- end }}
            templateRef:
              name: process-xml-release
              template: main
            arguments:
              parameters:
                - name: release-date
                  value: {{ $releaseDate | quote }}
                - name: gcs-prefix
                  value: {{ $gcsPrefix | quote }}

          - name: ingest-archive
            dependencies: [process-archive]
            {{- if $versionIsPinned }}
            when: {{ $processingNeeded | quote }}
            {{- end }}
            template: ingest-archive
            arguments:
              parameters:
                - name: release-date
                  value: {{ $releaseDate | quote }}
                - name: gcs-prefix
                  value: {{ $gcsPrefix | quote }}
            {{- $stagingDataset := "{{tasks.ingest-archive.outputs.parameters.dataset-name}}" }}

          # If we have provenance information and a snapshot doesn't exist, cut a snapshot of the new data
          # for the release-date.
          {{- if $versionIsPinned }}
          - name: check-latest-snapshot
            {{- include "argo.retry" . | indent 6 }}
            dependencies: [ingest-archive]
            template: check-latest-snapshot
          - name: publish-processing-results
            dependencies: [check-latest-snapshot]
            # only publish results when processing is needed and a snapshot is needed
            when: {{ "({{tasks.check-latest-snapshot.outputs.result}} == 0)" }}
            template: publish-processing-results
            arguments:
              parameters:
                - name: release-date
                  value: {{ $releaseDate | quote }}
                - name: gcs-prefix
                  value: {{ $gcsPrefix | quote }}
                - name: bq-dataset
                  value: {{ $stagingDataset | quote }}
          {{- end }}
      outputs:
        parameters:
          - name: dataset-name
            valueFrom:
              default: none
              parameter: {{ $stagingDataset | quote }}

    ##
    ## Sync the JSON outputs of Dataflow processing into the TDR.
    ##
    - name: ingest-archive
      ## TODO ARH testing, remove this notification
      onExit: send-slack-notification
      inputs:
        parameters:
          - name: gcs-prefix
          {{- $gcsPrefix := "{{inputs.parameters.gcs-prefix}}" }}
          - name: release-date
          {{- $releaseDate := "{{inputs.parameters.release-date}}" }}
      # Limit number of tables processed in parallel to avoid overwhelming
      # our cluster or the TDR.
      parallelism: {{ .Values.parallelism }}
      dag:
        tasks:
          # Create a dataset to store intermediate outputs of ETL jobs.
          - name: create-dataset
            templateRef:
              name: {{ .Values.argoTemplates.createBQDataset.name }}
              template: main
            arguments:
              parameters:
                - name: dataset-name
                  value: {{ printf "%s_%s" .Values.staging.bigquery.datasetPrefix $gcsPrefix | quote }}
                {{- with .Values.staging.bigquery }}
                - name: bq-project
                  value: {{ .project }}
                - name: dataset-description
                  value: {{ .description }}
                - name: dataset-expiration
                  value: {{ .expiration }}
                {{- end }}
            {{- $datasetName := "{{tasks.create-dataset.outputs.result}}" }}

          # Loop over all tables, syncing state in Jade to match data staged in GCS.
          - name: ingest-tables
            dependencies: [create-dataset]
            withItems:
              - clinical_assertion
              - clinical_assertion_observation
              - clinical_assertion_trait
              - clinical_assertion_trait_set
              - clinical_assertion_variation
              - gene
              - gene_association
              - rcv_accession
              - submission
              - submitter
              - trait
              - trait_mapping
              - trait_set
              - variation
              - variation_archive
            template: ingest-table
            arguments:
              parameters:
                - name: dataset-name
                  value: {{ $datasetName | quote }}
                - name: gcs-prefix
                  value: {{ $gcsPrefix | quote }}
                - name: table-name
                  value: {{ "{{item}}" | quote }}
                - name: release-date
                  value: {{ $releaseDate | quote }}
      outputs:
        parameters:
          - name: dataset-name
            valueFrom:
              parameter: {{ $datasetName | quote }}

    ##
    ## Sync the processed outputs of a single table into the TDR.
    ##
    - name: ingest-table
      inputs:
        parameters:
          - name: dataset-name
          {{- $datasetName := "{{inputs.parameters.dataset-name}}" }}
          - name: gcs-prefix
          {{- $gcsPrefix := "{{inputs.parameters.gcs-prefix}}" }}
          - name: table-name
          {{- $tableName := "{{inputs.parameters.table-name}}" }}
          - name: release-date
          {{- $releaseDate := "{{inputs.parameters.release-date}}" }}
      dag:
        tasks:
          # Diff staged data against previously-ingested state.
          {{- $newRowsPrefix := printf "%s/new-rows/%s" $gcsPrefix $tableName }}
          {{- $oldIdsPrefix := printf "%s/old-ids/%s" $gcsPrefix $tableName }}
          - name: diff-table
            templateRef:
              name: {{ .Values.argoTemplates.diffBQTable.name }}
              template: main
            arguments:
              parameters:
                - name: table-name
                  value: {{ $tableName | quote }}
                - name: gcs-bucket
                  value: {{ .Values.staging.gcsBucket }}
                - name: input-prefix
                  value: {{ printf "%s/processed/%s" $gcsPrefix $tableName | quote }}
                - name: old-ids-output-prefix
                  value: {{ $oldIdsPrefix | quote }}
                - name: new-rows-output-prefix
                  value: {{ $newRowsPrefix | quote }}
                - name: staging-bq-project
                  value: {{ .Values.staging.bigquery.project }}
                - name: staging-bq-dataset
                  value: {{ $datasetName | quote }}
                - name: jade-bq-project
                  value: {{ .Values.jade.dataProject }}
                - name: jade-bq-dataset
                  value: {{ printf "datarepo_%s" .Values.jade.datasetName }}
                - name: upsert
                  value: 'false'
                - name: diff-full-history
                  value: 'false'
                - name: jade-table-pre-filter
                  value: {{ printf "release_date = '%s'" $releaseDate | quote }}
            {{- $joinTable := "{{tasks.diff-table.outputs.parameters.join-table-name}}" }}
            {{- $shouldAppend := "{{tasks.diff-table.outputs.parameters.rows-to-append-count}} > 0" }}
            {{- $shouldDelete := "{{tasks.diff-table.outputs.parameters.ids-to-delete-count}} > 0" }}

          # Soft-delete IDs of outdated rows.
          - name: soft-delete-table
            dependencies: [diff-table]
            when: {{ $shouldDelete | quote }}
            templateRef:
              name: {{ .Values.argoTemplates.softDeleteTable.name }}
              template: main
            arguments:
              parameters:
                - name: table-name
                  value: {{ $tableName | quote }}
                - name: gcs-prefix
                  value: {{ $oldIdsPrefix | quote }}
                - name: gcs-bucket
                  value: {{ .Values.staging.gcsBucket }}
                {{- with .Values.jade }}
                - name: url
                  value: {{ .url }}
                - name: dataset-id
                  value: {{ .datasetId }}
                - name: timeout
                  value: {{ .pollTimeout }}
                - name: sa-secret
                  value: {{ .accessKey.secretName }}
                - name: sa-secret-key
                  value: {{ .accessKey.secretKey }}
                {{- end }}

          # Append rows with new data.
          - name: ingest-table
            dependencies: [diff-table, soft-delete-table]
            when: {{ $shouldAppend | quote }}
            templateRef:
              name:  {{ .Values.argoTemplates.ingestTable.name }}
              template: main
            arguments:
              parameters:
                - name: table-name
                  value: {{ $tableName | quote }}
                - name: gcs-bucket
                  value: {{ .Values.staging.gcsBucket }}
                - name: gcs-prefix
                  value: {{ $newRowsPrefix | quote }}
                {{- with .Values.jade }}
                - name: url
                  value: {{ .url }}
                - name: dataset-id
                  value: {{ .datasetId }}
                - name: timeout
                  value: {{ .pollTimeout }}
                - name: sa-secret
                  value: {{ .accessKey.secretName }}
                - name: sa-secret-key
                  value: {{ .accessKey.secretKey }}
                {{- end }}

    {{- if $versionIsPinned }}
    # check if the latest snapshot release date corresponds to the latest xml release date
    - name: check-latest-snapshot
      volumes:
        - name: sa-secret-volume
          secret:
            secretName: {{ .Values.jade.accessKey.secretName }}
      script:
        image: us.gcr.io/broad-dsp-gcr-public/monster-check-snapshot:0.1.4
        volumeMounts:
          - name: sa-secret-volume
            mountPath: /secret
        env:
          - name: GOOGLE_APPLICATION_CREDENTIALS
            value: {{ printf "/secret/%s" .Values.jade.accessKey.secretKey }}
          - name: REPO_DATA_PROJECT
            value: {{ .Values.jade.dataProject }}
          - name: REPO_DATASET_NAME
            value: {{ printf "datarepo_%s" .Values.jade.datasetName }}
          - name: REPO_HOST
            value: {{ .Values.jade.url }}
          - name: REPO_DATASET_ID
            value: {{ .Values.jade.datasetId }}
        command: ["python", "-c", "import check; check.run()"]

    ##
    ## Publish the data stored in the TDR under a release-date by marking
    ## it as processed with the current pipeline version, then cutting a
    ## snapshot.
    ##
    - name: publish-processing-results
      onExit: send-slack-notification
      inputs:
        parameters:
          - name: release-date
          {{- $releaseDate := "{{inputs.parameters.release-date}}" }}
          - name: gcs-prefix
          {{- $gcsPrefix := "{{inputs.parameters.gcs-prefix}}" }}
          - name: bq-dataset
          {{- $stagingDataset := "{{inputs.parameters.bq-dataset}}" }}
      dag:
        tasks:
          - name: stage-processing-history
            template: stage-processing-history-row
            arguments:
              parameters:
                - name: release-date
                  value: {{ $releaseDate | quote }}
                - name: gcs-prefix
                  value: {{ printf "%s/processed" $gcsPrefix | quote }}

          - name: sync-processing-history
            dependencies: [stage-processing-history]
            template: ingest-table
            arguments:
              parameters:
                - name: dataset-name
                  value: {{ $stagingDataset | quote }}
                - name: gcs-prefix
                  value: {{ $gcsPrefix | quote }}
                - name: table-name
                  value: processing_history
                - name: release-date
                  value: {{ $releaseDate | quote }}

          - name: submit-snapshot
            {{- include "argo.retry" . | indent 6 }}
            dependencies: [sync-processing-history]
            template: submit-snapshot
            arguments:
              parameters:
                - name: release-date
                  value: {{ $releaseDate | quote }}
            {{- $jobId := "{{tasks.submit-snapshot.outputs.result}}" }}

          - name: poll-snapshot
            {{- include "argo.retry" . | indent 6 }}
            dependencies: [submit-snapshot]
            template: poll-ingest-job
            arguments:
              parameters:
                - name: job-id
                  value: {{ $jobId | quote }}
                {{- with .Values.jade }}
                - name: api-url
                  value: {{ .url }}
                - name: timeout
                  value: {{ .pollTimeout }}
                - name: sa-secret
                  value: {{ .accessKey.secretName }}
                - name: sa-secret-key
                  value: {{ .accessKey.secretKey }}
                {{- end }}

    ##
    ## Count the number of rows in the processing_history table with a given release-date/version pair.
    ##
    - name: get-processing-history-rows
      inputs:
        parameters:
          - name: release-date
          {{- $releaseDate := "{{inputs.parameters.release-date}}" }}
      script:
        image: google/cloud-sdk:slim
        command: [bash]
        env:
          - name: PROJECT
            value: {{ .Values.staging.bigquery.project }}
          - name: JADE_PROJECT
            value: {{ .Values.jade.dataProject }}
          - name: JADE_DATASET
            value: {{ printf "datarepo_%s" .Values.jade.datasetName }}
          - name: RELEASE_DATE
            value: {{ $releaseDate | quote }}
          - name: VERSION
            value: {{ $pipelineVersion | quote }}
        source: |
        {{- include "argo.render-lines" (.Files.Lines "scripts/count-processing-history-rows.sh") | indent 10 }}

    ##
    ## Stage a JSON object in GCS containing data for a row in the processing_history table.
    ##
    - name: stage-processing-history-row
      inputs:
        parameters:
          - name: release-date
          {{- $releaseDate := "{{inputs.parameters.release-date}}" }}
          - name: gcs-prefix
          {{- $gcsPrefix := "{{inputs.parameters.gcs-prefix}}" }}
      script:
        image: google/cloud-sdk:slim
        command: [bash]
        {{- $uploadPath := printf "gs://%s/%s/processing_history/" .Values.staging.gcsBucket $gcsPrefix }}
        {{- $date := printf "%s-%s-%s" "{{workflow.creationTimestamp.Y}}" "{{workflow.creationTimestamp.m}}" "{{workflow.creationTimestamp.d}}"}}
        source: |
          proc_date=$(date '+%Y-%m-%d')
          json='{"release_date":"{{ $releaseDate }}", "pipeline_version":"{{ $pipelineVersion }}", "processing_date":"{{ $date }}"}'
          echo "$json" > history.json
          gsutil cp history.json {{ $uploadPath }}

    ##
    ## Submit a job to the TDR which will create a snapshot of all data for
    ## a given ClinVar release date.
    ##
    - name: submit-snapshot
      inputs:
        parameters:
          - name: release-date
            {{- $releaseDate := "{{inputs.parameters.release-date}}" }}
      volumes:
        {{- with .Values.jade }}
        - name: sa-secret-volume
          secret:
            secretName: {{ .accessKey.secretName }}
      script:
        image: us.gcr.io/broad-dsp-gcr-public/monster-auth-req-py:1.0.1
        volumeMounts:
          - name: sa-secret-volume
            mountPath: /secret
        env:
          - name: API_URL
            value: {{ .url }}
          - name: DATASET_NAME
            value: {{ .datasetName }}
          - name: PROFILE_ID
            value: {{ .profileId }}
          - name: GOOGLE_APPLICATION_CREDENTIALS
            value: {{ printf "/secret/%s" .accessKey.secretKey }}
          - name: ASSET_NAME
            value: {{ .snapshotAssetName }}
        {{- end }}
          - name: RELEASE_DATE
            value: {{ $releaseDate | quote }}
          - name: PIPELINE_VERSION
            value: {{ $pipelineVersion }}
          - name: ENVIRONMENT
            value: {{ .Values.jade.environment }}
        command: [python]
        source: |
        {{- include "argo.render-lines" (.Files.Lines "scripts/request-release-date-snapshot.py") | indent 10 }}

    ## Inject template used to poll TDR jobs.
    {{- include "argo.poll-ingest-job" . | indent 4 }}
    {{- end }}

      ##
      ## Send a Slack notification describing the final status of the workflow.
      ##
    - name: send-slack-notification
      container:
        image: curlimages/curl:7.70.0
        env:
          - name: WORKFLOW_ID
            value: {{ "{{workflow.name}}" | quote }}
          - name: WORKFLOW_ENV
            value: {{ .Values.jade.environment }}
          - name: SLACK_URL
            valueFrom:
              secretKeyRef:
                name: {{ .Values.notification.slackUrl.secretName }}
                key: {{ .Values.notification.slackUrl.secretKey }}
        command: [curl]
        args:
        - -XPOST
        - -H
        - 'Content-type: application/json'
        - -d
        - '{"text": "Workflow $(WORKFLOW_ID) in env *$(WORKFLOW_ENV)* has cut a new snapshot."}'
        - $(SLACK_URL)
